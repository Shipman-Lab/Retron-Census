{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "from collections import Counter\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq, reverse_complement\n",
    "import difflib\n",
    "import fuzzysearch\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globals\n",
    "wd = os.getcwd()\n",
    "user_profile = os.environ ['USERPROFILE']\n",
    "retrondb = '%s/retrondb_for_RTDNA_census.csv' % (wd) #for getting ncRNAs\n",
    "\n",
    "polyA_adapt = 'AAAAAAAAAAGATCGGAAGAGCACACGTCTGAACTCC'\n",
    "polyC_adapt = 'CCCCCCAGATCGGAAGAGCACACGTCTGAACTCC'\n",
    "\n",
    "sub = 6 #which subset to analyze\n",
    "pickled_globals = True #either False if it's the first subset or True if it's not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load retrondb from backup csv for ncRNAs\n",
    "rdb = pd.read_csv(retrondb)\n",
    "#load key of conditions and files\n",
    "samples = pd.read_excel(\"MiSeq_data_key_RTDNAseq.xls\")[[\"MiSeq file\",\"DBR1 status\",\"nodes\",\"subset\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "if pickled_globals == False:\n",
    "    node_list_with_repeats = []\n",
    "    for i in samples.index.values:\n",
    "        node_list_with_repeats.extend(list(map(int,samples.loc[i]['nodes'].split(','))))\n",
    "    node_list = list(set(node_list_with_repeats))\n",
    "\n",
    "    ncRNA_list = []\n",
    "    ncRNA_dict = {}\n",
    "    ncRNA_count = {}\n",
    "    ncRNA_count_rel_to_Eco1 = {}\n",
    "    for node in node_list:\n",
    "        ncRNA_list.append(rdb.loc[rdb.node == str(node),'ncRNA'].values[0])\n",
    "        ncRNA_dict[rdb.loc[rdb.node == str(node),'ncRNA'].values[0]] = node\n",
    "        ncRNA_count[node] = 0\n",
    "    coverage_dict_pA = {} #main key is node, under each node is a dictionary of base postition with counts of that base\n",
    "    for sequence in ncRNA_dict:\n",
    "        coverage_dict_pA[ncRNA_dict[sequence]] = {} #this will make a new dictionary for each node\n",
    "        for i in range(0,len(sequence)):\n",
    "            coverage_dict_pA[ncRNA_dict[sequence]][i] = 0\n",
    "\n",
    "    coverage_dict_pC = {} #main key is node, under each node is a dictionary of base postition with counts of that base\n",
    "    for sequence in ncRNA_dict:\n",
    "        coverage_dict_pC[ncRNA_dict[sequence]] = {} #this will make a new dictionary for each node\n",
    "        for i in range(0,len(sequence)):\n",
    "            coverage_dict_pC[ncRNA_dict[sequence]][i] = 0\n",
    "\n",
    "    coverage_dict_pA_minusDBR1 = {} #main key is node, under each node is a dictionary of base postition with counts of that base\n",
    "    for sequence in ncRNA_dict:\n",
    "        coverage_dict_pA_minusDBR1[ncRNA_dict[sequence]] = {} #this will make a new dictionary for each node\n",
    "        for i in range(0,len(sequence)):\n",
    "            coverage_dict_pA_minusDBR1[ncRNA_dict[sequence]][i] = 0\n",
    "\n",
    "    coverage_dict_pC_minusDBR1 = {} #main key is node, under each node is a dictionary of base postition with counts of that base\n",
    "    for sequence in ncRNA_dict:\n",
    "        coverage_dict_pC_minusDBR1[ncRNA_dict[sequence]] = {} #this will make a new dictionary for each node\n",
    "        for i in range(0,len(sequence)):\n",
    "            coverage_dict_pC_minusDBR1[ncRNA_dict[sequence]][i] = 0\n",
    "\n",
    "    fidelity_dict = {} #main key is node, under each node is a dictionary of total matched bases and incorrect bases (errors), includes both pA and pC\n",
    "    for sequence in ncRNA_dict:\n",
    "        fidelity_dict[ncRNA_dict[sequence]] = {'total_bases':0,'error_bases':0} #this will make a new dictionary for each node\n",
    "\n",
    "else:\n",
    "    node_list_with_repeats = []\n",
    "    for i in samples.index.values:\n",
    "        node_list_with_repeats.extend(list(map(int,samples.loc[i]['nodes'].split(','))))\n",
    "    node_list = list(set(node_list_with_repeats))\n",
    "\n",
    "    ncRNA_list = []\n",
    "    ncRNA_dict = {}\n",
    "    for node in node_list:\n",
    "        ncRNA_list.append(rdb.loc[rdb.node == str(node),'ncRNA'].values[0])\n",
    "        ncRNA_dict[rdb.loc[rdb.node == str(node),'ncRNA'].values[0]] = node\n",
    "    \n",
    "    with open('pickled_%s.pickle' %(str(sub-1)),\"rb\") as f: #note that this is based on the order they were written into the pickle\n",
    "        ncRNA_count = pickle.load(f)\n",
    "        ncRNA_count_rel_to_Eco1 = pickle.load(f)\n",
    "        coverage_dict_pA = pickle.load(f)\n",
    "        coverage_dict_pC = pickle.load(f)\n",
    "        coverage_dict_pA_minusDBR1 = pickle.load(f)\n",
    "        coverage_dict_pC_minusDBR1 = pickle.load(f)\n",
    "        fidelity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapter trimming and binning\n",
    "#input fastq\n",
    "#output total seqs, no_initial_T, no_R_adapter, just_ligated_adapters, RTDNA_shorter_than_10, pA_trimmed, pC_trimmed\n",
    "def adapter_trim(fastq):\n",
    "    seqs_in_sample = 0\n",
    "    just_ligated_adapters = []\n",
    "    RTDNA_shorter_than_10 = []  \n",
    "    no_initial_T = []\n",
    "    no_R_adapter = []\n",
    "    pA_trimmed = []\n",
    "    pC_trimmed = []\n",
    "    for seq_record in SeqIO.parse(fastq, \"fastq\"):\n",
    "        seqs_in_sample += 1\n",
    "        if str(seq_record.seq)[0] != 'T':\n",
    "            no_initial_T.append(seq_record)\n",
    "        else:\n",
    "            T_trimmed = str(seq_record.seq)[1:]\n",
    "            polyA_split = T_trimmed.split(polyA_adapt)\n",
    "            if len(polyA_split) == 2:\n",
    "                if polyA_split[0] == '':\n",
    "                    just_ligated_adapters.append(seq_record)\n",
    "                elif len(polyA_split[0]) < 11:\n",
    "                    RTDNA_shorter_than_10.append(seq_record)\n",
    "                else: \n",
    "                    pA_trimmed.append(polyA_split[0])\n",
    "            else:\n",
    "                polyC_split = T_trimmed.split(polyC_adapt)\n",
    "                if len(polyC_split) == 2:\n",
    "                    if polyC_split[0] == '':\n",
    "                        just_ligated_adapters.append(seq_record)\n",
    "                    elif len(polyC_split[0]) < 11:\n",
    "                        RTDNA_shorter_than_10.append(seq_record)\n",
    "                    else: \n",
    "                        pC_trimmed.append(polyC_split[0])\n",
    "                else: no_R_adapter.append(seq_record) \n",
    "    return (seqs_in_sample, no_initial_T, no_R_adapter, just_ligated_adapters, RTDNA_shorter_than_10, pA_trimmed, pC_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin by retron, just pA\n",
    "def match_and_analyze(DBR1_status, A_or_C, nodes, counter):\n",
    "    internal_ncRNA_list = []\n",
    "    ncRNA_count_pA = {}\n",
    "    for node in nodes:\n",
    "        internal_ncRNA_list.append(rdb.loc[rdb.node == str(node),'ncRNA'].values[0])\n",
    "        ncRNA_count_pA[node] = 0\n",
    "    unmatched_count = 0\n",
    "    one_matched_count = 0\n",
    "    double_matched_count = 0\n",
    "    odd_bucket = 0\n",
    "    binned_to_node_but_no_fuzzy_match = []\n",
    "    global coverage_dict_pA\n",
    "    global coverage_dict_pC\n",
    "    global coverage_dict_pA_minusDBR1\n",
    "    global coverage_dict_pC_minusDBR1\n",
    "    global fidelity_dict\n",
    "    global ncRNA_count\n",
    "    if DBR1_status == 'DBR1+':\n",
    "        if A_or_C == 'A':\n",
    "            coverage_dict = coverage_dict_pA\n",
    "        else: \n",
    "            coverage_dict = coverage_dict_pC\n",
    "    else:\n",
    "        if A_or_C == 'A':\n",
    "            coverage_dict = coverage_dict_pA_minusDBR1\n",
    "        else: \n",
    "            coverage_dict = coverage_dict_pC_minusDBR1      \n",
    "    for key in counter:\n",
    "        R_seq = reverse_complement(key) #reverse complement to put it in the same dirction as the ncRNA\n",
    "        ncRNA_match = difflib.get_close_matches(R_seq,internal_ncRNA_list)\n",
    "        if len(ncRNA_match) == 1:  #this is the case we care about where it found one unique match in the ncRNA list\n",
    "            node = ncRNA_dict[ncRNA_match[0]]\n",
    "            ncRNA_count[node] += counter[key]\n",
    "            one_matched_count += counter[key]\n",
    "            match_loc = fuzzysearch.find_near_matches(R_seq, ncRNA_match[0], max_l_dist=int(len(R_seq)*0.1)) #allowing 10% mismatch, too high?\n",
    "            if len(match_loc): #some sequences are found with difflib, but don't match closely enough to work with fuzzysearch\n",
    "                for i in range(match_loc[0].start,match_loc[0].end):\n",
    "                    coverage_dict[node][i] += counter[key]\n",
    "                if DBR1_status == 'DBR1+':  #only use the DBR1 data for fidelity\n",
    "                    fidelity_dict[node]['total_bases'] += (match_loc[0].end-match_loc[0].start)*counter[key]\n",
    "                    fidelity_dict[node]['error_bases'] += match_loc[0].dist*counter[key]\n",
    "                    if A_or_C == 'A':\n",
    "                        ncRNA_count_pA[node] += counter[key] #only use pA, DBR1 data for counts relative to Eco1\n",
    "            else:\n",
    "                binned_to_node_but_no_fuzzy_match.append(key)\n",
    "        elif len(ncRNA_match) > 1:\n",
    "            double_matched_count += counter[key]\n",
    "        elif len(ncRNA_match) < 1:\n",
    "            unmatched_count += counter[key]\n",
    "        else: odd_bucket += 1\n",
    "    if DBR1_status == 'DBR1+' and A_or_C == 'A': \n",
    "        return (unmatched_count,one_matched_count,double_matched_count,odd_bucket,binned_to_node_but_no_fuzzy_match,ncRNA_count_pA)\n",
    "    else:    \n",
    "        return (unmatched_count,one_matched_count,double_matched_count,odd_bucket,binned_to_node_but_no_fuzzy_match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = samples.loc[samples['subset'] == sub]\n",
    "\n",
    "for i in sub_df.index.values:\n",
    "    fastq_reads = '%s/%s.fastq' % (wd, samples.loc[i]['MiSeq file'])\n",
    "    seqs_in_sample, no_initial_T, no_R_adapter, just_ligated_adapters, RTDNA_shorter_than_10, pA_trimmed, pC_trimmed = adapter_trim(fastq_reads)\n",
    "    print (samples.loc[i]['MiSeq file'])\n",
    "    print ('Total Seqs: %s' % (seqs_in_sample))\n",
    "    print ('pA_trimmed: %s' % len(pA_trimmed))\n",
    "    print ('pC_trimmed: %s' % len(pC_trimmed))\n",
    "    print ('just_ligated_adapters: %s' % len(just_ligated_adapters))\n",
    "    print ('RTDNA_shorter_than_10: %s' % len(RTDNA_shorter_than_10))\n",
    "    print ('no_initial_T: %s' % len(no_initial_T))\n",
    "    print ('no_R_adapter: %s' % len(no_R_adapter) + '\\n')\n",
    "    pA_counter = Counter(pA_trimmed)\n",
    "    pC_counter = Counter(pC_trimmed)\n",
    "    #analyze pA\n",
    "    if samples.loc[i][1] == 'DBR1+':\n",
    "        unmatched_count,one_matched_count,double_matched_count,odd_bucket,binned_to_node_but_no_fuzzy_match,ncRNA_count_pA = match_and_analyze(samples.loc[i][1], 'A', list(map(int,samples.loc[i]['nodes'].split(','))), pA_counter)\n",
    "        #deal with relative counts \n",
    "        for node in ncRNA_count_pA:\n",
    "            current = float(ncRNA_count_pA[node])/ncRNA_count_pA[1550]\n",
    "            try:\n",
    "                prev = ncRNA_count_rel_to_Eco1[node]\n",
    "                ncRNA_count_rel_to_Eco1[node] = (prev+current)/2\n",
    "            except KeyError:\n",
    "                ncRNA_count_rel_to_Eco1[node] = current\n",
    "    else:\n",
    "        unmatched_count,one_matched_count,double_matched_count,odd_bucket,binned_to_node_but_no_fuzzy_match = match_and_analyze(samples.loc[i][1], 'A', list(map(int,samples.loc[i]['nodes'].split(','))), pA_counter)   \n",
    "    print ('pA reads')\n",
    "    print ('unmatched count: %s' % (unmatched_count))\n",
    "    print ('one match count: %s' % (one_matched_count))\n",
    "    print ('double matched count: %s' % (double_matched_count))\n",
    "    print ('binned to node but no fuzzy match: %s' % (len(binned_to_node_but_no_fuzzy_match)))\n",
    "    print ('odd leftovers: %s' % (odd_bucket) + '\\n')\n",
    "    #analyze pC\n",
    "    unmatched_count,one_matched_count,double_matched_count,odd_bucket,binned_to_node_but_no_fuzzy_match = match_and_analyze(samples.loc[i][1], 'C', list(map(int,samples.loc[i]['nodes'].split(','))), pC_counter)\n",
    "    print ('pC reads')\n",
    "    print ('unmatched count: %s' % (unmatched_count))\n",
    "    print ('one match count: %s' % (one_matched_count))\n",
    "    print ('double matched count: %s' % (double_matched_count))\n",
    "    print ('binned to node but no fuzzy match: %s' % (len(binned_to_node_but_no_fuzzy_match)))\n",
    "    print ('odd leftovers: %s' % (odd_bucket) + '\\n')\n",
    "\n",
    "with open('pickled_%s.pickle' %(str(sub)),\"wb\") as f:\n",
    "    pickle.dump(ncRNA_count,f)\n",
    "    pickle.dump(ncRNA_count_rel_to_Eco1,f)\n",
    "    pickle.dump(coverage_dict_pA,f)\n",
    "    pickle.dump(coverage_dict_pC,f)\n",
    "    pickle.dump(coverage_dict_pA_minusDBR1,f)\n",
    "    pickle.dump(coverage_dict_pC_minusDBR1,f)\n",
    "    pickle.dump(fidelity_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe for output, only run this after the final subset\n",
    "coverage_dict_pA_minusDBR1\n",
    "output_data = {'node':[],\n",
    "               'RTDNAseq_count_pA':[],\n",
    "               'RTDNAseq_count_pC':[],\n",
    "               'RTDNAseq_count_pA_minusDBR1':[],\n",
    "               'RTDNAseq_count_pC_minusDBR1':[],\n",
    "               'RTDNAseq_errors_per_base':[],\n",
    "               'RTDNAseq_count_rel_to_Eco1':[]}              \n",
    "for node in node_list:\n",
    "    output_data['node'].append(node)\n",
    "    cov_list_pA = [str(coverage_dict_pA[node][key]) for key in sorted(coverage_dict_pA[node].keys())]\n",
    "    cov_list_pC = [str(coverage_dict_pC[node][key]) for key in sorted(coverage_dict_pC[node].keys())]\n",
    "    cov_list_pA_minusDBR1 = [str(coverage_dict_pA_minusDBR1[node][key]) for key in sorted(coverage_dict_pA_minusDBR1[node].keys())]\n",
    "    cov_list_pC_minusDBR1 = [str(coverage_dict_pC_minusDBR1[node][key]) for key in sorted(coverage_dict_pC_minusDBR1[node].keys())]    \n",
    "    output_data['RTDNAseq_count_pA'].append(\";\".join(cov_list_pA))\n",
    "    output_data['RTDNAseq_count_pC'].append(\";\".join(cov_list_pC))\n",
    "    output_data['RTDNAseq_count_pA_minusDBR1'].append(\";\".join(cov_list_pA_minusDBR1))\n",
    "    output_data['RTDNAseq_count_pC_minusDBR1'].append(\";\".join(cov_list_pC_minusDBR1))\n",
    "    output_data['RTDNAseq_count_rel_to_Eco1'].append(ncRNA_count_rel_to_Eco1[node])\n",
    "    if fidelity_dict[node]['total_bases'] > 0:  \n",
    "        output_data['RTDNAseq_errors_per_base'].append(fidelity_dict[node]['error_bases']/fidelity_dict[node]['total_bases'])\n",
    "    else: output_data['RTDNAseq_errors_per_base'].append('div/0')\n",
    "RTDNA_seq_analysis = pd.DataFrame(output_data)\n",
    "RTDNA_seq_analysis.to_csv(\"RTDNA_seq_analysis.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
